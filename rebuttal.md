We sincerely thank all reviewers for their comments. 

**Reviewer 1**

1. It is not clear why the model has been branded as an Android defense mechanism. The model should be applicable to other domains too where adversarial sample generation is easy (e.g., PDF malwares). Please have a look at- "Automatically Evading Classifiers" by Xu et al. Does the model work superior for only the Android malware dataset?

**Response**: Our model is applicable to the domains using discrete features for detection, including PDF malware detection. 

2. A better discussion on whether the adversarial sample generation techniques and the alternative defense mechanisms (AR, RT etc.) are state-of-the-art was necessary.

**Response:** Thanks a lot. The white-box attack methods used in our manuscript have been widely used in top conference/journal (e.g., [1]) papers. The black-box attack methods MalGAN and E-MalGAN are recently proposed. To our knowledge, E-MalGAN is the latest black-box attack method for Android malware detectors. The defense mechanisms such as AR and RT have also been used in top conference/journal (CVPR 2020, TIFS 2020 and Usenix Security 2020) papers. 

[1] Adversarial Deep Ensemble: Evasion Attacks and Defenses for Malware Detection.tifs.2020.

3. Related work section needs to be rewritten. Other researches related to adversarial attacks and defenses could be pointed out. The current discussion presents a brief description of a few alternative defense mechanisms. These could be moved to the preliminaries section.

**Response:** Thanks. We will rewrite the section of related work and add more descriptions in the revised manuscript.

4. The reasoning behind threshold(v) selection in Sec. 4.5 is a bit vague. Could it be more formally described?

**Response:** Thanks. In this section, we select a desired threshold through evaluating all candidate thresholds in malware detection and AE detection. Fig. 9 reflects how these thresholds perform in distinguishing malware from begin apps. Fig. 10 shows how they perform in identifying AEs and normal examples. It can be seen that the threshold 30 achieves satisfactory FA, MD, and detection accuracies. Therefore, this threshold is chosen by us. In the revised manuscript, we will rewrite this section to make it clearer. 

5. A brief description of the adversarial sample generation techniques- dFGSMk, rFGSMk, BGAk, BCAk, and APDNN could be helpful.

**Response**: Thanks. We will revise our manuscript according to this comment.

6. The paper should follow the convention of referring to the sections using their numbers and not titles.

**Response**: Thanks for your comment. We are sorry to make this mistake and we will use number to refer a section in the next version.

**Reviewer 2**

1. The features for the ML models are the "traditional" ones used in many works (e.g., Drebin). However it's not clear whether the specific features are chosen. For example, the paper mentions that only 106 sensitive APIs are used as features. How were they chosen? There are several works that focus one extracting reasonable lists of APIs (either based on permission usaged or other facotrs). Did the authors use these existing resources?

**Response**:

Thanks a lot. Similar to [1], we choose 379 features including permissions, intent actions and API calls. In fact, we conducted many experiments over different feature sets, and finally selected them due to their excellent performance. We show the details of these features and describe how to extract these features from an apk file in https://github.com/Henry-0811/Robust-Android-Malware-Detection-Against-Adversarial. 

[1] A Lightweight On-device Detection Method for Android Malware. IEEE Trans. SMC: Systems, 2019.

2. One concern I have is that there is no comparison with some important related works and several very related works are not even cited. For example, the paper does not discuss the "decreased" (?) accuracy of previous techniques when tasked to detect adversarial samples. The paper does discuss several different models, but no comparison is offered with popular previuos tools such as DREBIN and Mamadroid. What would be their accuracy?

**Response**:

We really appreciate this comment. The main idea of our manuscript is to study malware detection and AE defense in a joint fashion. DREBIN and Mamaroid are excellent detectors, but they do not consider AE defense. In addition, both malware detection and AE generation heavily depend on the features. Since DREBIN and Mamadroid use the features different from ours, it is hard to directly compare them with our method. 

3. There are also several works in a very similar area that are not even mentioned. A couple of example that deal with Android malware adversarial samples generation + protection:
   \- "Adversarial Examples for Malware Detection", ESORICS17
   \- "Using Loops For Malware Classification Resilient to Feature-unaware Perturbations", ACSAC18

   None of these works seem a "complete overlap", but it may be beneficial to add some discussion and context.

**Response**: Thanks. We will add the discussions on them into the revised manuscript.

4. This paper seems a bit out of scope for what concerns the WWW CFP. I don't see any link with web security or any of the topic listed in the CFP.

**Response**: Thanks. Before submitting our manuscript, we found several www 2020 papers that study Android malware and adversarial examples. For example, the best student paper of www 2020 is “Mobile App Squatting”, which focuses on Android malware and explores the squatting attacks in Android apps.

5. There is no mention of any code or dataset release. It would be beneficial if, at least, the authors could make public the list of permissions/intent/APIs used as features.

**Response**:

The code and dataset will be released later. The list of permissions/intent/APIs used as features is shown at  https://github.com/Henry-0811/Robust-Android-Malware-Detection-Against-Adversarial. 

6. Minor: there are many typos and spelling errors.

**Response**:

We will improve the presentation quality of our manuscript.

**Reviewer 3**

1. [Q1] Was any obfuscation present in the malicious training data examples?

**Response**: Thanks for your careful consideration. In our experiments, no adversarial examples (AEs) were chosen as training data. There is no obfuscation in the malicious training data examples. In addition, no overfitting is found in our experiments. We will release the code and dataset in github.

2. [Q2] If obfuscation was present in the training data examples, could this obfuscation have been achieved by one of the proposed attacks (malGAN or E-MalGAN)?

   [Q2] If MalGAN and E-MalGAN generate more effective adversarial examples with more rounds, how are the defense mechanisms detecting them with a higher probability as these rounds increase? This is counterintuitive as the GAN should have improved the quality of adversarial examples.

**Response**: We really appreciate this comment. 

1) The existence of malware AEs is due to the fact that the decision boundary of a target classifier is imperfect. If the decision boundary is very close to the perfect or ideal boundary, generating AEs becomes extremely difficult. 

2) In a GAN, Generator and Discriminator keep evolving only when the balance between them is well maintained. If Discriminator dominates Generator, GAN may finally walk to collapse and cannot produce good samples. 

3) When defense mechanisms are adopted, the decision boundary of target classifiers (e.g., rFGSM_RT) becomes closer to the ideal boundary. Under this situation, finding AEs becomes very difficult for Generator. Therefore, Generator is dominated by Discriminator, making GANs (e.g., MalGAN) finally walk to collapse and produce poor samples.   

4) Why can E-MalGAN beat the classical defense mechanisms (e.g., AR and RT)? To generate adversarial examples (AEs) that can circumvent malware detection, MalGAN employs one discriminator to simulate its target classifier. In addition to simulating its target classifier, E-MalGAN also adopts another discriminator to distinguish the generated AEs from natural examples. Aided by the second discriminator, E-MalGAN can generate the AEs highly similar to natural examples. Therefore, E-MalGAN succeeds in cheating the existing classifiers even if they are equipped with defense mechanisms. But E-MalGAN cannot cheat our classifier, because our decision boundary is very close to the ideal boundary due to the introduction of feature disentangle. Accordingly, E-MalGAN cannot find the appropriate perturbations making its samples stride over the decision boundary. 

5) Why does our method performs better on E-MalGAN than on MalGAN? As mentioned above, E-MalGAN generates the AEs highly similar to natural examples. Experiments show that E-MalGAN generates much smaller perturbations than MalGAN does. However, the pursuit of smaller perturbations makes E-MalGAN weaker in cheating classifiers. More specifically, smaller perturbations make E-MalGAN harder to stride over the decision boundary of our classifier. This accounts for why E-MalGAN is easily defended by our method. Furthermore, our method does not detect AEs at all. Therefore, the main consideration of E-MalGAN (i.e., evading AE detection) becomes useless for our method.

3. [Q3] What are the types of behavior of the malware analyzed in this paper? Is the malware analyzed applications that illegitimately use permissions, if so, how does this work compare with [1]? Is the malware analyzed in applications exploiting any specific vulnerabilities? 

**Response:**

Thanks. [1] points out an interesting direction which detects apps by judging whether the behavior of ICONS in an app is the same as the permissions required for it. Our method does not require running an app, and hence it does not use the icon information. So we do not compare our method with [1]. 

4.The evaluation section requires a better organization, and the related work section did not provide a clear point as to how the proposed mechanism differentiates from previous work. 

**Response**: Thanks a lot. We will rewrite the sections of evaluation and related work in the revised paper. 

5. In Fig. 6, the red dotted line is not included.

**Response**: Thanks. The red dotted line does exists, but is not easy to find. In the revised paper, we will redraw this figure and strengthen this line. 

 